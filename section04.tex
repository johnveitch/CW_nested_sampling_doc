\section{Evaluating the code}

In this section we will show how the posterior parameter distributions that we can generate from the output
of our nested sampling code compare to evaluating them on a grid, or with a previous implementation of the analysis code
\citep[\lppe used in e.g.][]{2014ApJ...785..119A} using a (generally inefficient) MCMC algorithm. We will show this for
simulated data containing purely Gaussian noise and containing fake signals added to Gaussian noise. We will also
show this for some hardware injections (signals directly induced in detector data) in real LIGO data from the fifth
science run (S5).

We will also directly compare some S5 results for searches for real signals from pulsars with those previously published
\citep{2010ApJ...713..671A}.

Finally we show how the odds comparing a signal model and a noise only model change with signal-to-noise ratio, along
with comparing coherent verses incoherent signals (for coherent simulated signal and incoherent simulations).

Add p-p plots!

\subsection{Simulated data}

In each of the tests below we will be running our code (\lppen) using the default proposal distributions discussed in \S\ref{sec:proposals}
and with the number of live points fixed to be 2048. We will also assume a source at a fixed sky position and that the signal model is purely
that for the $l=m=2$ harmonic (Equation~\ref{eq:h2f}), and that we work in terms of the signal amplitude $h_0$ and signal phase
$\Phi_{22}^C$ (this is for consistency with the old code which uses $\Phi_{22}^C$ rather than $\phi_0$, which we have earlier defined as the
rotational phase). The simulated data in all cases 
will be generated as the complex heterodyned
time series sampled once per minute. When comparisons between the current code and previous codes are 
made we run the current code in a way that splits datasets into 30 point chunks, rather than using the algorithm described in \S{sec:splitting},
as this makes it consistent with the old code (\lppe). We also purely use the uniform prior ranges for the amplitude parameter to be
consistent between codes.

\subsubsection{Simulated noise}\label{sec:simnoise}

An initial test of the code is whether the output posterior probability distributions match those produced when evaluating the
posterior over a fixed grid in the four-dimensional parameter space of $\vec{\theta} = \{h_0, \cos{\iota}, \psi, \phi_{22}^C\}$, when
using purely Gaussian noise.\footnote{It is worth noting that in many of these tests the polarisation angle prior covers $-\pi/4$ to $\pi/4$
rather than the 0 to $\pi/2$ range given in, e.g., \citet{2015MNRAS.453.4399P} due to this being the range required by the older
parameter estimation MCMC code. Although this range is degenerate to rotations over $\pi/2$, so spans the same range of waveform models.}
A comparison of the marginalised posteriors for individual parameters, and pairs of parameters, are shown when using simulated data lasting
ten days from a single detector (in this case assumed to be the LIGO Hanford detector, H1) in Figure~\ref{fig:simnoise_single}.

It can be seen that the output posteriors look qualititavely consistent. However, we can also quantify some aspects of consistency by
comparing the evidence output from the nested sampling code with that estimated from the grid-based method from \lppe, the upper limits
on $h_0$ produced by the codes, and performing Kolmogorov-Smirnov consistency tests between the nested sampling posterior samples
and MCMC posterior samples (giving a $p$-value for the null hypothesis that the samples are from the same distributions).
These are shown in Table~\ref{tab:codeeval} and show very good consistency between the codes, although it should be noted that
these are for one particular run and some statistical fluctuations in the exact values for different runs are to be expected (see e.g.\
the variations in evidence values in \S\ref{sec:proposaltesting}).

A {\tt jupyter} notebook with this test can be found \href{https://github.com/mattpitkin/CW_nested_sampling_doc/blob/master/figures/codeeval/simulations/noise/SimulatedNoiseTestsPaper.ipynb}{here}.

\begin{table*}[h]
\caption{Consistency tests between outputs of the new code, \lppen, and the old code, \lppe, when running on simulated data
and searching over the four parameters $\{h_0, \cos{\iota}, \psi, \Phi_{22}^C\}$.\label{tab:codeeval}}
\begin{center}
\begin{tabular}{l c c | c c c c}
\hline
\multirow{2}{*}{Simulation} & \multirow{2}{*}{$\ln{\left(\frac{Z_{\rm nested}}{Z_{\rm grid}}\right)}$} & \multirow{2}{*}{$\frac{(h_0^{95\%})_{\rm nested}}{(h_0^{95\%})_{\rm grid}}$} & 
\multicolumn{4}{c}{K-S $p$-value} \\ \cline{4-7}
 &  &  & $p(h_0)$ & $p(\phi_0)$ & $p(\cos{\iota})$ & $p(\psi)$ \\                      
\hline
\hline
Noise (single detector)  & $-0.07$ & $1.009$ & 0.404 & 0.026 & 0.010 & 0.703 \\
Noise (two detectors)    & $-0.05$ & $0.992$ & 0.141 & 0.564 & 0.538 & 0.493 \\
Signal (single detector) & 0.245   & $1.004$ & 0.237 & 0.291 & 0.125 & 0.175 \\
Signal (two detectors)   & 0.240   & 0.991   & 0.722 & 0.052 & 0.067 & 0.032 \\
\hline
\end{tabular}
\end{center}
\end{table*}

We see a very similar situation, in terms of agreement between the codes, when running on simulated data assumed to be from two detectors (the LIGO
H1 and L1 sites) as shown in Figure~\ref{fig:simnoise_single} and the second line of Table~\ref{tab:codeeval}. A {\tt jupyter} notebook with this test
can be found \href{https://github.com/mattpitkin/CW_nested_sampling_doc/blob/master/figures/codeeval/simulations/noise_multidet/SimulatedNoiseTestsMultidetPaper.ipynb}{here}.

\subsubsection{Simulated signal}

The next test is checking how the codes compare when there is a signal present in the data. When generating the simulated signals we initially
assume that the signal's phase evolution is perfectly known and has been removed via the heterodyne described in \S\ref{sec:model}, and therefore
$\Delta\phi_2 = 0$ from Equation~\ref{eq:deltaphi}. As in \S\ref{sec:simnoise} then will search over the four-dimensional parameter space of
$\vec{\theta} = \{h_0, \cos{\iota}, \psi, \phi_{22}^C\}$.

We create a simulated signal in the H1 detector spanning ten days of data with parameters ($h_0 = 6.2\ee{-23}$, $\Phi_{22}^C = 2.4$\,rad, $\cos{\iota} = 0.3$
and $\psi = 0.1$\,rad) that produce a signal-to-noise ratio of 8 when injected into
Gaussian noise. The posterior probability distributions estimated for the parameters are shown in Figure~\ref{fig:simsignal_single}, which show consistency
between the old and new codes and with the injected signal parameters. The quantitative consistency between the codes can be seen in Table~\ref{tab:codeeval},
where is should be noted that the evidence ratio between the codes is larger than statistical uncertainty would expect, although the fractional difference between
the values ($\lesssim 1\%$) is still small enough to not greatly effect conclusions drawn from either values if used in model selection.

Similarly, using a signal with a coherent multi-detector signal-to-noise ratio of 8 (for the same parameters as above except with $h_0 = 3.5\ee{-23}$) when
simulated and injected into Gaussian noise for two detectors (H1
and L1) we see the posteriors shown in Figure~\ref{fig:simsignal_multi}, and consistency checks in Table~\ref{tab:codeeval}.

The notebooks for the above two tests can be found \href{https://github.com/mattpitkin/CW_nested_sampling_doc/blob/master/figures/codeeval/simulations/signal/SimulatedSignalTestsPaper.ipynb}{here} and 
\href{https://github.com/mattpitkin/CW_nested_sampling_doc/blob/master/figures/codeeval/simulations/signal_multidet/SimulatedSignalMultidetTestsPaper.ipynb}{here}.
