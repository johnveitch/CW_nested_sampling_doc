\documentclass[aps,prd,showpacs,superscriptaddress,twocolumn,preprintnumbers,altaffilletter]{revtex4-1}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{cleveref}

\newcommand{\ee}[1]{\!\times\!10^{#1}}
\newcommand{\gws}{gravitational waves\xspace}
\newcommand{\gw}{gravitational wave\xspace}
\newcommand{\lalinf}{\texttt{LALInference}\xspace}

\bibliographystyle{aipauth4-1}

\begin{document}

\title{A nested sampling code for targeted searches for continuous \gws from pulsars}

\author{M.~Pitkin}\affiliation{SUPA, School of Physics \& Astronomy, University of Glasgow, Glasgow, G12 8QQ,
United Kingdom}
\author{J.~Veitch}\affiliation{School of Physics \& Astronomy, University of Birmingham, Birmingham, B15 2TT,
United Kingdom}
\author{G.~Woan}\affiliation{SUPA, School of Physics \& Astronomy, University of Glasgow, Glasgow, G12 8QQ,
United Kingdom}

\email{matthew.pitkin@glasgow.ac.uk}

\date{\today}

\begin{abstract}
This document will detail a code to perform parameter estimation and model selection in targeted searches for
continuous \gws from known pulsars. We will describe the general workings of the code along with characterising
it on simulated data and real LIGO S5 data. We will also show how it performs compared to a previous MCMC and
grid-based approach to signal parameter estimation. An appendix will detail how to run the code in a variety
of cases.
\end{abstract}

\pacs{01.50.ht, 02.50.Tt, 04.80.Nn}
\preprint{}

\maketitle

\section{Overview}

One of the primary methods used in searches for \gws from known pulsars is based on two stages: a heterodyne
stage that pre-processes the calibrated \gw detector data by removing a signal's expected intrinsic phase
evolution (based on the observed solution from electromagnetic observations), low-pass filtering and
down-sampling (via averaging) \cite{2005PhRvD..72j2002D}; and, a parameter estimation stage that takes the
processed data and uses it to estimate posterior probability distributions of the unknown signal parameters
(e.g.\ using a Markov chain Monte Carlo (MCMC) \cite{2010ApJ...713..671A}). This method has been variously
called the {\it Time Domain Method}, the {\it Heterodyne Method}, the {\it Bayesian Method}, the {\it Glasgow
Method}, or some combination of these names. Up to now this method has only been used to set upper limits
on signal amplitudes, but has had no explicit criteria for deciding on signal detection/significance. A further
limitation has been that the MCMC method used was not designed to be efficient and required tuning. It also
did not straighforwardly have the ability to perform a search on the data over small ranges in the phase
evolution parameters (e.g.\ required if the
\gw signal does not well match the known pulsar's phase evolution). For these reasons the parameter
estimation code has been re-written to use the nested sampling algorithm \cite{Skilling:2006}, in particular
a method based on that described in \cite{Veitch:2010}. This allows us to evaluate the {\it evidence} or {\it
marginal likelihood} for the signal model and compare it to other model evidences (i.e.\ the data is just
Gaussian noise), whilst also giving posterior probabilty distributions for the signal parameters.

This code makes use of the nested sampling algorithm provided within the \lalinf software library
\citep{2015PhRvD..91d2003V}. As such this document will not give a detailed description of how that algorithm
works, but will provide some information on the specific proposals that can be used within the algorithm.
This method has previously been briefly described in \cite{2012JPhCS.363a2041P}.

The code can also be used to perform parameter estimation and model selection for signals for any generic
metric theory of gravity, however its use for this will be discussed in detail in a separate paper. When
searching over parameters that vary the phase evolution there is potential to speed up the code through
efficient likelihood evaluation via the {\it reduced order quadrature} method, but again that will be
discussed in more detail in a future paper.

\subsection{General knowledge}

The code calculates the Bayesian evidence, or {\it marginal likelihood} for a particular signal model. The
evidence, $\mathcal{Z}$, for a given model, $M$, defined by a set of parameters, $\vec{\theta}$ is given by
\begin{equation}\label{eq:evidence}
\mathcal{Z}_M = p(d|M,I) = \int^{\vec{\theta}} p(d|\vec{\theta},M,I)p(\vec{\theta}|M,I) {\rm d}\vec{\theta},
\end{equation}
where $p(d|\vec{\theta},M)$ is the likelihood function for the data $d$ given the
model and its set of defining parameters and $p(\vec{\theta}|M,I)$ is the prior probability distribution on
$\vec{\theta}$. During nested sampling this integral is evaluated by transforming it into the one dimensional
sum
\begin{equation}\label{eq:nestedsampev}
\mathcal{Z}_M = \sum_i^N L_i w_i,
\end{equation}
where $L_i$ is the likelihood and $w_i = p(\vec{\theta}|M,I) \Delta\vec{\theta}$ is the ``prior weight''
(i.e.\ inverse prior volume occupied by point $i$).

As a default the signal model evidence is compared to the
evidence that the data consists of Gaussian noise to form an odds ratio for the two models
\begin{equation}\label{eq:oddsratio}
\mathcal{O} = \frac{p(d|M,I)}{p(d|\text{noise},I)}\frac{p(M|I)}{p(\text{noise}|I)} =
\frac{\mathcal{Z}_M}{\mathcal{Z}_{\text{noise}}},
\end{equation}
where on the right hand side we have explicitly set the prior odds for the two models to
$p(M|I)/p(\text{noise}|I) = 1$.

Other than this evidence value and odds ratio the code will also output all of the samples accumlated during
the nested sampling process. These samples will not be drawn from the posterior probability distribution as in
an MCMC method, but they can be resampled to generate a subset of samples that are drawn from the posteior
distribution. This resampling (performed using the {\tt lalapps\_nest2pos} {\tt python} script within
{\tt lalapps}\footnote{\url{https://www.lsc-group.phys.uwm.edu/daswg/projects/lalsuite.html}}) uses the
value of $L_i w_i$ for each point, normalised by $(L_iw_i)_{\rm max}$ to give values proportional to the
posterior probability, and the accepts a point with a probability given by its value, i.e.\ \ point is
accepted with the probability
\begin{equation}
P_{\text{accept}} = \frac{L_i w_i}{(L_iw_i)_{\rm max}}.
\end{equation}


In our case the data $d$ in the above equations is heterodyned, low-pass filtered and down-sampled data
for a detector, or set of detectors, each giving a single data stream or two data streams depending on
whether the heterodyne was performed at one of both potential signal frequencies near the rotation frequency
and/or twice the rotation frequency. Here we will use $\mathbf{B}$ to represent a vector of these heterodyned
data values, for which a single time sample is often refered to as $B_k$ (``{\it B of k}''), although we will
not consistly use $k$ as the time index for the data.

\section{Core functions}

Here we will describe the core parts of the code defining the signal model and the probability functions used
for the inference. These assume that the data consists of calibrated narrow-band complex heteroyned time
series'. These time series data streams can be from different detectors, and/or different heterodyne
frequencies. For example you could have a pair times series from both the LIGO Hanford (H1) and LIGO
Livingston (L1) detectors, with one produced with a heterodyne at twice the rotatation frequency of a known
pulsar and the other produced with a heterodyne at the rotation frequency.

\subsection{The signal model}

Our code assumes that the rotational phase evolution of a pulsar is defined by the Taylor expansion in phase
\begin{equation}
\phi(t) = 2\pi\left(fT + \frac{1}{2}\dot{f}T^2 + \frac{1}{6}\ddot{f}T^3  \ldots\right)
\end{equation}
where $T$ is the time corrected to an inertial reference frame (the solar system barycentre
for isolated pulsars, or the binary system barycentre for pulsars in binaries), and the $f$'s give
the rotation frequency and its time derivatives. The value of $T = (t+\tau(t)-t_0)$ where $t$ is the
time at a detector, $\tau(t)$ is the time dependent correction to the inertial frame and $t_0$ is the epoch.
In practice the code can currently accept frequency derivatives up to the fifth order. We assume the
calibrated detector data $d(t)$ has been heterodyned such that
\begin{equation}
B'(t) = d(t)e^{-i\Phi_{i,\rm het}(t)},
\end{equation}
where $\Phi_{i,\rm het}(t)$ is the phase evolution for a given data stream $i$, and we produce $B(t)$ by
low-pass filtering and averaging values of $B'(t)$.

Under the standard assumption that the general theory of relativity (GR) is correct the code uses the form of
the signal model defined in \cite{2015arXiv150105832J} that, when heterodyned and asumming low-pass
filtered, gives a signal at a pulsar's rotation frequency (where $\Phi_{1,{\rm het}}(t) = \phi(t)$) of
\begin{widetext}
\begin{equation}\label{eq:hf}
h_f(t) =  e^{i\Delta\phi_1(t)}\left(-\frac{C_{21}}{4}F_{+}(\psi,t)\sin{\iota}\cos{\iota}e^{i\Phi_{21}^C} +
i\frac{C_{21}}{4}F_{\times}(\psi,t)\sin{\iota}e^{i\Phi_{21}^C} \right)
\end{equation}
and at twice the pulsar's rotation frequency (where $\Phi_{2,{\rm het}}(t) = 2\phi(t)$() of
\begin{equation}\label{eq:h2f}
h_{2f}(t) =  e^{i\Delta\phi_2(t)}\left(-\frac{C_{22}}{2}F_{+}(\psi,t)[1+\cos{}^2\iota]e^{i\Phi_{22}^C} +
iC_{22}F_{\times}(\psi,t)\cos{\iota}e^{i\Phi_{22}^C} \right).
\end{equation}
\end{widetext}
The $F_{+}$ and $F_{\times}$ values are the detector dependent antenna patterns, which are a function of the
detector position, source sky position and source polarisation angle $\psi$. The $C_{21}$, $C_{22}$,
$\Phi_{21}^C$ and $\Phi_{22}^C$ values are convenient ways of representing the waveform in terms of an
amplitude and phase of the signal for the $l=2$, $m=1$ harmonic and $l=m=2$ harmonic respectively. The
$\Delta\phi(t)$ values represents any time dependent phase deviation between the phase used in the heterodyne
and the true signal phase (which does not necessarily have to precisely follow the true rotational phase), so
$\Delta\phi_1(t) = (\phi_{1,{\rm true}}(t)-\Phi_{1,{\rm het}}(t))$ and $\Delta\phi_2(t) = (\phi_{2,{\rm
true}}(t)-\Phi_{2,{\rm het}}(t))$.

By default the code assumes emission just from the $l=m=2$ mode, i.e.\ there is only a signal at twice the
rotation frequency. In this case $C_{22}$ and $\Phi_{22}^C$ can be related to the more familiar physical
$h_0$ and $\phi_0$ values via $h_0 = 2C_{22}$ and $\phi_0 = \Phi_{22}^C/2$. For the more general case the
relations between the waveform amplitude and phase parameters and physical source parameters are given in
\cite{2015arXiv150105832J}. In general for previous searches we have often assumed that we track the true
signal phase perfectly with the heterodyne and as such $\Delta\phi_i(t) = 0$. In such cases the only time
varying components of the signal are the antenna pattern functions, which allows great speed increases in the
signal generation and likelihood calculations.

\subsection{The likelihood functions}\label{sec:likelihood}

Our code can make use of two different likelihood functions. The default likelihood function is a
Students-{\it t} likelihood function in which it is assumed that the standard devaition of the noise in the
data is unknown, and can therefore be marginalised over. A Gaussian likelihood function can also be used, for
which the code can either take in estimates of the noise standard devaition at each data point, or calculates
these internally based on stationary stretches of data. For the Students-{\it t} likelihood function, and if
calculated noise standard deviations for the Gaussian likelihood function internally the code needs to break
up the data into chunks that have (roughly) the same distribution. The method for doing this is given in
Section~\ref{sec:splitting}.

\subsubsection{Students-{\it t} likelihood}

A full derivative of the Students-{\it t} likelihood function (see e.g.~\cite{2005PhRvD..72j2002D}) is given
in Appendix~\ref{app:likelihood}, but the final form of the joint likelihood (and its natural logarithm,
which is actually used within the code to maintain precision) for multiple detectors and data streams is
given by
\begin{widetext}
\begin{align}\label{eq:stlikelihood}
p(\mathbf{B}|\vec{\theta}) &= \prod_{i=1}^{N_{\rm dets}} \prod_{j=1}^{N_{\rm s}} \prod_{k=1}^{M_{i,j}}
\frac{(m_{i,j,k}-1)!}{2\pi^{m_{i,j,k}}}
\left(\sum_{n=n_{i,j,0}}^{n_{i,j,0}+(m_{i,j,k}-1)} |B_{i,j,n}-y(\vec{\theta})_{i,j,n}|^2\right)^{-m_{i,j,k}},
\nonumber \\
\ln{p(\mathbf{B}|\vec{\theta})} &= \sum_{i=1}^{N_{\rm dets}} \sum_{j=1}^{N_{\rm s}}
\sum_{k=1}^{M_{i,j}} \left( \mathcal{A}_{i,j,k} - m_{i,j,k}\ln{
\left\{\sum_{n=n_{i,j,0}}^{n_{i,j,0}+(m_{i,j,k}-1)} |B_{i,j,n}-y(\vec{\theta})_{i,j,n}|^2\right\}}
\right),
\end{align}
\end{widetext}
where $N_{\rm dets}$ is the number of detectors used, $N_{\rm s}$ is the number of data streams (e.g.\
heterodyned data from both the rotation frequency and twice the rotation frequency) per detector, $M_{i,j}$ is
the total number of independent data chunks for detector $i$ and data stream $j$ with lengths $m_{i,j,k}$ and
$n_{i,j,0} = \sum_{l=1}^{k} 1+m_{i,j,l-1}$ (with $m_{i,j,0} = 0$) being the index of the first data point in
each chunk. The model $y(\vec{\theta})$ is that given by Eqns.~\ref{eq:hf} and/or \ref{eq:h2f}
depending on which data streams are being analysed. For notational convenience we have made the substitution
$\mathcal{A}_{i,j,k} = \ln{\left([m_{i,j,k}-1]!\right)} - \ln{2} - m_{i,j,k}\ln{\pi}$.

\subsubsection{Gaussian likelihood}

The Gaussian likelihood, and its natural logarithm, are similarly given by
\begin{widetext}
\begin{align}\label{eq:gausslikelihood}
p(\mathbf{B}|\vec{\theta}) &= \prod_{i=1}^{N_{\rm dets}} \prod_{j=1}^{N_{\rm s}} \prod_{k=1}^{L_{i,j}}
\frac{1}{2\pi\sigma_{i,j,k}^2}\exp{\left(-\frac{|B_{i,j,k}-y(\vec{\theta})_{i,j,k}|^2}{2\sigma_{i,j,k}^2}
\right)}, \nonumber \\
\ln{p(\mathbf{B}|\vec{\theta})} &= \sum_{i=1}^{N_{\rm dets}} \sum_{j=1}^{N_{\rm s}}
\sum_{k=1}^{L_{i,j}} \left(\mathcal{B}_{i,j,k} -
\left[\frac{|B_{i,j,k}-y(\vec{\theta})_{i,j,k}|^2}{2\sigma_{i,j,k}^2 } \right] \right)
\end{align}
\end{widetext}
where $L_{i,j}$ is the length of each dataset and $\mathcal{B}_{i,j,k} = -\ln{(2\pi\sigma_{i,j,k}^2)}$. Note
that the normal square root on the normalisation factor is not there because the exponential is already the
product of the real and imaginary data components.

\subsubsection{The null likelihood}

As we will most often want to perform model comparison for the signal model against the data just containing
noise (the null hypothesis in this case) we can define the null likelihoods for both of the above likelihoods
by setting $y(\vec{\theta}) = 0$, so that we have
\begin{align}
\ln{p(\mathbf{B}|y=0)} &= \sum_{i=1}^{N_{\rm dets}} \sum_{j=1}^{N_{\rm s}}
\sum_{k=1}^{M_{i,j}} \Bigg( \mathcal{A}_{i,j,k} - \nonumber \\
&m_{i,j,k}\ln{
\left\{\sum_{n=n_{i,j,0}}^{n_{i,j,0}+(m_{i,j,k}-1)} |B_{i,j,n}|^2\right\}}
\Bigg),
\end{align}
and
\begin{equation}
\ln{p(\mathbf{B}|y=0)} = \sum_{i=1}^{N_{\rm dets}} \sum_{j=1}^{N_{\rm s}}
\sum_{k=1}^{L_{i,j}} \left(\mathcal{B}_{i,j,k} -
\left[\frac{|B_{i,j,k}|^2}{2\sigma_{i,j,k}^2 } \right] \right)
\end{equation}
for the Students-{\it t} and Gaussian likelihoods respectively.

If we were only interested in comparing models calculated using equivalent likelihood functions we could in
general ignore the factors that do not depend on the data or the model, as they would cancel in any odds
ratio. But, in this code we keep them for cases when such a comparison is not performed, e.g.\ if we
want to compared the joint multi-detector likelihood for a signal with the incoherent sum of likelihoods
from each detector then we would need these factors to still be present.

\subsubsection{Fast likelihood evaluations}

In cases when the only time varying components of the model are the antenna pattern functions the likelihood
evalution can be greatly sped-up by precalculated the components in the internal summations. For a given sky
position and detector the antenna patterns can be defined by \cite{1998PhRvD..58f3001J}
\begin{align}
F_+(t,\psi) &= \sin{\zeta}\left[a(t)\cos{2\psi} + b(t)\sin{2\psi}\right], \nonumber \\
F_{\times}(t,\psi) &= \sin{\zeta}\left[b(t)\cos{2\psi} - b(t)\sin{2\psi}\right],
\end{align}
where $\psi$ is the \gw polarisation angle, $zeta$ is the known angle between the detector arms (generally
$90^{\circ}$), and $a(t)$ and $b(t)$ are the time dependent functions for a given detector position and
source sky location that vary over a sidereal day. These functions can be precomputed at a set of times over
a sidereal day to uses for look-up table interpolation to give the value at any other time (our code
defaults to calculate $a(t)$ and $b(t)$ at 2880 points over a sidereal day). If, for example, we take the
real part of the cross polarisation component of a signal model, then the summation in the likelihood for a
single detector (with $\zeta = 90^{\circ}$), single data stream and single chunk of length $n$ will be given
by
\begin{widetext}
\begin{align}
S =& \sum_{i=1}^n (\Im{(B_i)}-\left[a(t_i)\cos{2\psi} -
b(t_i)\sin{2\psi}\right]\mathcal{C})^2 \nonumber \\
 =& \sum_{i=1}^n \Im{(B_i)}^2 + \mathcal{C}^2\sum_{i=1}^n \left[a(t_i)\cos{2\psi} -
b(t_i)\sin{2\psi}\right]^2 - 2\mathcal{C}\sum_{i=1}^n  \Im{(B_i)}\left[a(t_i)\cos{2\psi} -
b(t_i)\sin{2\psi}\right], \nonumber \\
=& \sum_{i=1}^n \Im{(B_i)}^2 + \mathcal{C}^2\cos{}^2{2\psi}\sum_{i=1}^n a(t_i)^2 +
\mathcal{C}^2\sin{}^2{2\psi}\sum_{i=1}^n b(t_i)^2 - 2\mathcal{C}^2\cos{2\psi}\sin{2\psi}\sum_{i=1}^n
a(t_i)b(t_i) - \nonumber \\
& 2\mathcal{C}\cos{2\psi} \sum_{i=1}^n \Im{(B_i)}a(t_i) + 2\mathcal{C}\sin{2\psi} \sum_{i=1}^n
\Im{(B_i)}b(t_i),
\end{align}
\end{widetext}
where $\mathcal{C}$ represents the imaginary component of waveform amplitude for a given set of parameters.
It can be seen that all the summation terms can be pre-computed. If using the Gaussian likelihood this
pre-computation of the summation terms can also be done, but with the substitions $B_i \rightarrow
B_i/\sigma_i$, $a(t_i) \rightarrow a(t_i)/\sigma_i$ and $b(t_i) \rightarrow b(t_i)/\sigma_i$. For the model,
assuming GR emission just from the $l=m=2$ mode, it is just the four different $\mathcal{C}$ terms that need
to be calculated during the sampling process within the code.

In the case where we want to search over parameters that mean that $\Delta\phi_i(t) \ne 0$ in the model then
we can not perform this pre-summing. However, {\it reduced order quadrature} methods (e.g.\
\cite{2014PhRvX...4c1006F, 2015PhRvL.114g1104C}) may help in these cases. Such a method is implemented in our
code, but will be discussed in a separate paper.

\subsection{The prior functions}

\subsection{Splitting the data}\label{sec:splitting}

The above likelihood functions require us to have an idea about the timescales on which the data is
stationary, i.e.\ periods when the noise in the data is best described as being drawn from a single
Gaussian distribution. We therefore use a scheme similar to the BayesianBlocks change point algorithm
\cite{1998ApJ...504..405S} to find points at which the statistics of the noise changes.

\section{Evaluating the parameter estimation}

\section{Evaluating the evidence calculation}

\subsection{Simulated Gaussian noise}

\subsection{LIGO S5 data}

\subsubsection{Evaluating coherent signals}

We can run the code such that it provides the joint evidence for a signal in multiple detectors, but we can
also run it for individual detectors. This allows us to compare the model that there is a coherent signal
between detectors (as would be expected for a real signal) to the model that the signal is incoherent between
detectors (as you might expect from a detector line artefact). This can be done by comparing the joint
analysis evidence to the sum of the individual detector evidences (i.e.\ the logical {\it or} between
detectors)
\begin{equation}
\mathcal{O}_{\rm coherent} = \frac{\mathcal{Z}_{\rm Joint}}{\sum_{i=1}^{N_{\rm det}} \mathcal{Z}_{{\rm det}_i}}
\end{equation}

\appendix

\section{Derivation of the Students-{\it t} likelihood function}\label{app:likelihood}

Here we derive in detail the form of the Students-{\it t} likelihood given in Section~\ref{sec:likelihood}.
This derivation can in part be found in \cite{Dupuisthesis} and \cite{2005PhRvD..72j2002D}, but we correct a
slight error from those references. If a stretch of data of length $m$ is assumed to consist of a signal
defined by a set of parameter $\vec{\theta}$ and Gaussian noise with zero mean and standard deviation
$\sigma$, then the standard deviation can be included as an unknown parameter in the likelihood function and
marginalised over. The likelihood can therefore be given by
\begin{equation}\label{eq:likesigma}
p(\mathbf{B}|\vec{\theta}) = \int_0^{\infty} p(\mathbf{B},\sigma|\vec{\theta}) {\rm d}\sigma =
\int_0^{\infty} p(\mathbf{B}|\vec{\theta},\sigma)p(\sigma) {\rm d}\sigma,
\end{equation}
where $p(\mathbf{B}|\vec{\theta},\sigma)$ is the likelihood function for the data given the unknown signal
parameters and the noise standard deviation, and $p(\sigma)$ is the prior on $\sigma$.

Given that the noise is assumed Gaussian the likelihood within the integral is given by
\begin{equation}\label{eq:likesigma2}
p(\mathbf{B}|\vec{\theta},\sigma) = \left(\frac{1}{2\pi\sigma^2}\right)^{m}
\exp{\left( -\frac{\sum_{k=1}^m|B_k-y(\vec{\theta})_k|^2}{2\sigma^2} \right)}.
\end{equation}
for our complex data $B$ and model $y$, where for a complex number $x$ we have used $|x|^2 =
\Re{(x)}^2+\Im{(x)}^2$, and the noise in the real and imaginary parts is assumed to have the same
distribution.

We chose a scale invariant prior on $\sigma$ of $p(\sigma) = 1/\sigma$ (for $\sigma > 0$, otherwise it is
zero), for which the marginalisation of Eqn.~\ref{eq:likesigma} can be performed analytically as we will show.
Subtituting this prior and Eqn.~\ref{eq:likesigma2} into Eqn.~\ref{eq:likesigma} we can begin the
integration with the substitution
\begin{equation}\label{eq:u}
u = \sqrt{\frac{\sum_{k=1}^m|B_k-y(\vec{\theta})_k|^2}{2\sigma^2}},
\end{equation}
giving
\begin{equation}\label{eq:du}
\frac{{\rm d}u}{{\rm d}\sigma} = -\sqrt{\frac{\sum_{k=1}^m|B_k-y(\mathbf{a})_k|^2}{2}}\sigma^{-2}.
\end{equation}
Here we note that Eqn.~\ref{eq:du} is different from the equivalent Eqn.~2.28 in \cite{Dupuisthesis} by a
factor of two. Rearranging Eqns.~\ref{eq:u} and~\ref{eq:du}, and for simplicity using the substitution $X =
\sum_{k=1}^m |B_k-y(\vec{\theta})_k|^2$, gives
\begin{equation}
\sigma = \left(\frac{X}{2u^2}\right)^{1/2},
\end{equation}
and
\begin{align}
{\rm d}\sigma = & -\sigma^2 \left(\frac{2}{X}\right)^{1/2} {\rm d}u
\nonumber \\
 = & -\frac{X}{2u^2} \left(\frac{2}{X}\right)^{1/2} {\rm d}u \nonumber \\
 = & - 2^{-1/2} X^{1/2} u^{-2} {\rm d}u.
\end{align}
Putting everything into Eqn.~\ref{eq:likesigma} gives
\begin{widetext}
\begin{align}\label{eq:integral1}
p(\mathbf{B}|\vec{\theta})= & -(2\pi)^{-m} \int_{\infty}^0 2^{(m+\frac{1}{2})}
X^{-(m+\frac{1}{2})} u^{2m+1} e^{-u^2} 2^{-1/2} X^{1/2} u^{-2} {\rm d}u, \nonumber \\
 = & -\pi^{-m} X^{-m} \int_{\infty}^0 u^{2m-1} e^{-u^2} {\rm d}u,
\end{align}
\end{widetext}
where the substitution has changed the integral from being $\int_0^{\infty}$ to $\int_{\infty}^0$. The
integral in Eqn.~\ref{eq:integral1} can then be performed by making the substitutions $x = u^2$ and ${\rm d}x
= 2u\,{\rm d}u$, giving
\begin{equation}
p(\mathbf{B}|\vec{\theta}) = - \pi^{-m} X^{-m} \int_{\infty}^0 \frac{1}{2} x^{m-1} e^{-x}
{\rm d}x.
\end{equation}
This integral is the definition of the Gamma function
\begin{equation}
 \int_{\infty}^0 x^{m-1} e^{-x} {\rm d}x = -\Gamma(m).
\end{equation}
For positive integers Stirling's formula shows that the Gamma function is given by $\Gamma(n+1) = n!$, so our
likelihood becomes
\begin{equation}\label{eq:complex}
p(\mathbf{B}|\vec{\theta}) = \frac{(m-1)!}{2\pi^m} \left(\sum_{k=1}^m
|B_k-y(\vec{\theta})_k|^2\right)^{-m}.
\end{equation}
Note that, again, this differs from Eqn.~2.31 in \cite{Dupuisthesis} by some constant factors.

For our analysis the data is split into chunks for each of which the assumption of noise stationarity is
thought to be valid, but between chunks is thought to be invalid. The joint likelihood of all data can be
obtained from the product of the likelihood for each chunk given by
\begin{equation}\label{eq:prod}
p(\mathbf{B}|\vec{\theta}) = \prod_{j=1}^M \frac{(m_j-1)!}{2\pi^{m_j}}
\left(\sum_{k=k_0}^{k_0+(m_j-1)} |B_k-y(\vec{\theta})_k|^2\right)^{-m_j},
\end{equation}
where $M$ is the total number of independent data chunks with lengths $m_j$ and $k_0 = \sum_{i=1}^{j}
1+m_{i-1}$ (with $m_0 = 0$) is the index of the first data point in each chunk.

\section{Code usage}

Here we document the various command line options for the code and its use in a variety of situations.

\bibliography{nested_sampling_doc}

\end{document}